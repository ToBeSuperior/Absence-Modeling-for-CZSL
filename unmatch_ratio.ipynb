{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "#  Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Python imports\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torchvision.models as tmodels\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import join as ospj\n",
    "import itertools\n",
    "import glob\n",
    "import random\n",
    "\n",
    "#Local imports\n",
    "from data import dataset as dset\n",
    "from models.common import Evaluator\n",
    "from models.image_extractor import get_image_extractor\n",
    "from models.manifold_methods import RedWine, LabelEmbedPlus, AttributeOperator\n",
    "from models.modular_methods import GatedGeneralNN\n",
    "from models.symnet import Symnet\n",
    "from utils.utils import save_args, UnNormalizer, load_args\n",
    "from utils.config_model import configure_model\n",
    "from flags import parser\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import easydict\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'logs/co-cge-ow/mitstates/'\n",
    "yml = path+'mit.yml'\n",
    "ck = path+'ckpt_best_auc.t7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# args format\n",
    "\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \n",
    "    'config': 'configs/co-cge/mit_anno.yml',\n",
    "    'dataset': 'mitstates',\n",
    "    'data_dir': 'mit-states',\n",
    "    'logpath': path,           \n",
    "    'splitname': 'compositional-split-natural',\n",
    "    'cv_dir': 'logs/',\n",
    "    'name': path,\n",
    "    'load': None,\n",
    "    'image_extractor': 'resnet18',\n",
    "    'norm_family': 'imagenet',\n",
    "    'num_negs': 1,\n",
    "    'pair_dropout': 0.0,\n",
    "    'test_set': 'val',\n",
    "    'clean_only': False,\n",
    "    'subset': False,\n",
    "    'open_world': True,\n",
    "    'test_batch_size': 128,\n",
    "    'cpu_eval': True,\n",
    "\n",
    "    'model': 'graphfull',\n",
    "    'emb_dim': 512,\n",
    "    'nlayers': 2,\n",
    "    'nmods': 24,\n",
    "    'embed_rank': 64,\n",
    "    'bias': 1e3,\n",
    "    'update_features': True,\n",
    "    'freeze_features': False,\n",
    "    # 'use_feature': True,\n",
    "    'emb_init': 'ft+w2v',\n",
    "    'clf_init': False,\n",
    "    'static_inp': False,\n",
    "    'composition': 'mlp_add',\n",
    "    # 'relu': True,\n",
    "    'dropout': True,\n",
    "    'norm': True,\n",
    "    'train_only': False,\n",
    "    'train_triplet_loss': False,\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    'fast_eval': True,\n",
    "    'closed_eval': None,\n",
    "    \n",
    "    # Model parameters\n",
    "    'train_only': False,\n",
    "\n",
    "    #CGE\n",
    "    'graph': False,\n",
    "    'graph_init': None,\n",
    "    'gcn_type': 'gcn',\n",
    "\n",
    "    # Forward\n",
    "    'eval_type': 'dist',\n",
    "\n",
    "    # Primitive-based loss\n",
    "    'lambda_aux': 0.0,\n",
    "\n",
    "    # AoP\n",
    "    'lambda_inv': 0.0,\n",
    "    'lambda_comm': 0.0,\n",
    "    'lambda_ant': 0.0,\n",
    "\n",
    "\n",
    "    # SymNet\n",
    "    'lambda_trip': 0,\n",
    "    'lambda_sym': 0,\n",
    "    'lambda_axiom': 0,\n",
    "    'lambda_cls_attr': 0,\n",
    "    'lambda_cls_obj': 0,\n",
    "\n",
    "    # CompCos (for the margin, see below)\n",
    "    'cosine_scale': 50,\n",
    "    'epoch_max_margin': 100,\n",
    "    'update_feasibility_every': 1,\n",
    "    'hard_masking': False,\n",
    "    'threshold': None,\n",
    "    'threshold_trials': 50,\n",
    "\n",
    "    # Graph methods\n",
    "    'graph_init': None,\n",
    "    'gcn_type': 'gcn',\n",
    "    'gr_emb': 'd600',\n",
    "    'cosine_classifier': True,\n",
    "    'feasibility_adjacency': True,\n",
    "\n",
    "    # Hyperparameters\n",
    "    'topk': 3,\n",
    "    'margin': 1.0,\n",
    "    'workers': 8,\n",
    "    'batch_size': 128,\n",
    "    'lr': 5e-5,\n",
    "    'lrg': 1e-3,\n",
    "    'wd': 5e-5,\n",
    "    'save_every': 10000,\n",
    "    'eval_val_every': 1,\n",
    "    'max_epochs': 200,\n",
    "    'fc_emb': '768,1024',\n",
    "    'gr_emb': 'd4096',\n",
    "    'fast_eval': True\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run one of the cells to load the dataset you want to run test for and move to the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ut = yml    # logs/unmatch/cgqa_2/cgqa_unmatch_2.yml\n",
    "load_args(best_ut,args)\n",
    "args.graph_init = args.graph_init\n",
    "args.load = ck"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading arguments and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all pairs\n",
      "Dataset loaded\n",
      "Train pairs: 1262, Validation pairs: 600, Test Pairs: 800\n",
      "Train images: 30328, Validation images: 10417, Test images: 12988\n",
      "Objs  245  Attrs  115\n"
     ]
    }
   ],
   "source": [
    "from flags import DATA_FOLDER\n",
    "\n",
    "args.test_set = 'test'\n",
    "testset = dset.CompositionDataset(\n",
    "        root= os.path.join(DATA_FOLDER,args.data_dir),\n",
    "        phase=args.test_set,\n",
    "        split=args.splitname,\n",
    "        model =args.image_extractor,\n",
    "        subset=args.subset,\n",
    "        return_images = True,\n",
    "        update_features = args.update_features,\n",
    "        open_world=args.open_world,\n",
    "        # clean_only = args.clean_only\n",
    "    )\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers)\n",
    "\n",
    "print('Objs ', len(testset.objs), ' Attrs ', len(testset.attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext Embeddings loaded, total embeddings: torch.Size([360, 300])\n",
      "Word2Vec Embeddings loaded, total embeddings: torch.Size([360, 300])\n",
      "Combined embeddings are  torch.Size([360, 600])\n",
      "Learnable image_embeddings\n",
      "Evaluating with test pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/czsl/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/czsl/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "image_extractor, model, optimizer = configure_model(args, testset)\n",
    "evaluator = Evaluator(testset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from  logs/co-cge-ow/mitstates_4/ckpt_best_auc.t7\n",
      "Best AUC:  0.027498053712842557\n"
     ]
    }
   ],
   "source": [
    "if args.load is not None:\n",
    "    checkpoint = torch.load(args.load)\n",
    "    if image_extractor:\n",
    "        try:\n",
    "            image_extractor.load_state_dict(checkpoint['image_extractor'])\n",
    "            image_extractor.eval()\n",
    "        except:\n",
    "            print('No Image extractor in checkpoint')\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "    model.eval()\n",
    "    print('Loaded model from ', args.load)\n",
    "    print('Best AUC: ', checkpoint['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = sorted(list(set(testset.train_pairs + testset.val_pairs + testset.test_pairs)))\n",
    "seen_match = sorted(list(set(testset.train_pairs)))\n",
    "unseen_match = sorted(list(set(testset.val_pairs + testset.test_pairs) - set(testset.train_pairs)))\n",
    "unseen_unmatch = sorted(list(set(testset.pairs) - set(closed)))\n",
    "open = sorted(list(set(testset.pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmatch_ratio(scores, exp, flag=False):\n",
    "    \n",
    "    result = scores[exp]\n",
    "    attr = [evaluator.dset.attrs[result[0][idx,a]] for a in range(topk)]\n",
    "    obj = [evaluator.dset.objs[result[1][idx,a]] for a in range(topk)]\n",
    "    attr_gt, obj_gt = evaluator.dset.attrs[data[1][idx]], evaluator.dset.objs[data[2][idx]]\n",
    "    \n",
    "   \n",
    "    prediction = ''\n",
    "    seen_num=0\n",
    "    unseen_num=0\n",
    "    unmatch = 0\n",
    "    for i, (a,o) in enumerate(zip(attr, obj)):\n",
    "        p_space = 'closed' if (a, o) in closed else 'open'\n",
    "        if (a,o) in seen_match:\n",
    "            seen_num += 1\n",
    "        elif (a,o) in unseen_match:\n",
    "            unseen_num += 1\n",
    "        else:\n",
    "            unmatch += 1\n",
    "            \n",
    "    return seen_num, unseen_num, unmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatch pair candidate ratio\n",
      "seen unmatch ratio:  0.7767031118587048  /  0.30571909167367534\n",
      "unseen unmatch ratio:  0.7558718190386428  /  0.29585296889726675\n",
      "total unmatch ratio:  0.7596858638743456  /  0.2976593778872806\n"
     ]
    }
   ],
   "source": [
    "total_candidate_num = 5 * len(testset)\n",
    "seen_candidate_num = 0\n",
    "unseen_candidate_num = 0\n",
    "unmatch_candidate_num = 0\n",
    "\n",
    "for index, data in enumerate(testloader):\n",
    "    images = data[-1]\n",
    "    data = [d.to(device) for d in data[:-1]]\n",
    "    if image_extractor:\n",
    "        data[0] = image_extractor(data[0])\n",
    "    _,  predictions, _ = model(data)\n",
    "    data = [d.to('cpu') for d in data]\n",
    "    topk = 5\n",
    "    results = evaluator.score_model(predictions, data[2], bias = 1000, topk=topk)\n",
    "\n",
    "    printed_lsit = []\n",
    "    for idx in range(len(images)):\n",
    "        seen = bool(evaluator.seen_mask[data[3][idx]])\n",
    "\n",
    "        sm, um, uu = unmatch_ratio(results, 'open')\n",
    "        seen_candidate_num += sm\n",
    "        unseen_candidate_num += um\n",
    "        unmatch_candidate_num += uu\n",
    "\n",
    "print(\"unmatch pair candidate ratio\")\n",
    "\n",
    "print(\"total ratio: \", str(seen_candidate_num/total_candidate_num), \" / \" , str(unseen_candidate_num/total_candidate_num), \" / \" ,str(unmatch_candidate_num/total_candidate_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
